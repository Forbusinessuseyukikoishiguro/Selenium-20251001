
ã•ã‚‰ã«è©³ã—ãèª¬æ˜ã—ã¾ã™ï¼

## ğŸ”¥ ä¸¡æ–¹ã®ãƒ¡ãƒªãƒƒãƒˆã‚’æ´»ã‹ã™æ–¹æ³•

å®Œç’§ãª**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ„ãƒ¼ãƒ«**ã‚’ä½œæˆã—ã¾ã—ãŸï¼ğŸ‰

## ğŸŒŸ ã“ã®ãƒ„ãƒ¼ãƒ«ã®ç‰¹å¾´

### 1. **è‡ªå‹•åˆ¤å®šãƒ¢ãƒ¼ãƒ‰** ğŸ¤–
```python
scraper = HybridScraper(use_selenium=None)  # è‡ªå‹•åˆ¤å®š
scraper.fetch(url)
```

å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†ï¼š
1. ã¾ãšé«˜é€Ÿãª`requests`ã§è©¦ã™
2. HTMLã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆJavaScriptãŒå¿…è¦ã‹åˆ¤å®šï¼‰
3. å¿…è¦ãªã‚‰è‡ªå‹•ã§`Selenium`ã«åˆ‡ã‚Šæ›¿ãˆ

### 2. **3ã¤ã®ãƒ¢ãƒ¼ãƒ‰**

| ãƒ¢ãƒ¼ãƒ‰ | è¨­å®š | ä½¿ã„ã©ã“ã‚ |
|--------|------|------------|
| è‡ªå‹•åˆ¤å®š | `use_selenium=None` | åŸºæœ¬ã“ã‚Œã§OK |
| Seleniumå¼·åˆ¶ | `use_selenium=True` | JavaScriptç¢ºå®Ÿã«å¿…è¦ |
| requestså¼·åˆ¶ | `use_selenium=False` | é™çš„HTMLã®ã¿ï¼ˆé«˜é€Ÿï¼‰ |

---

## ğŸ“¦ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```bash
# ã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install selenium requests beautifulsoup4 lxml webdriver-manager

# webdriver-managerã‚’ä½¿ã†å ´åˆï¼ˆæ¨å¥¨ï¼‰
pip install webdriver-manager
```

---

## ğŸš€ ä½¿ã„æ–¹

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from hybrid_scraping_tool import HybridScraper

# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆï¼ˆè‡ªå‹•åˆ¤å®šãƒ¢ãƒ¼ãƒ‰ï¼‰
scraper = HybridScraper()

# 2. URLã‚’å–å¾—
scraper.fetch('https://example.com')

# 3. è¦ç´ ã‚’æ¤œç´¢
titles = scraper.find_all('.article-title')

# 4. ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
config = {
    'container': '.article',  # è¨˜äº‹ã®è¦ªè¦ç´ 
    'fields': {
        'title': 'h2',        # ã‚¿ã‚¤ãƒˆãƒ«
        'link': 'a',          # ãƒªãƒ³ã‚¯
        'date': '.date'       # æ—¥ä»˜
    }
}

data = scraper.extract_data(config)

# 5. ä¿å­˜
scraper.save_to_csv(data)
scraper.save_to_json(data)

# 6. çµ‚äº†
scraper.close()
```

---

## ğŸ’¡ å®Ÿè·µä¾‹

### ä¾‹1: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆè‡ªå‹•åˆ¤å®šï¼‰

```bash
python hybrid_scraping_tool.py
# 1ã‚’é¸æŠ
# URL: https://news-site.com
# ã‚³ãƒ³ãƒ†ãƒŠ: .article-item
# ã‚¿ã‚¤ãƒˆãƒ«: .title
# ãƒªãƒ³ã‚¯: a
```

### ä¾‹2: ECã‚µã‚¤ãƒˆï¼ˆSeleniumå¼·åˆ¶ï¼‰

```bash
python hybrid_scraping_tool.py
# 2ã‚’é¸æŠ
# URL: https://shop-site.com
# (è‡ªå‹•ã§å•†å“æƒ…å ±ã‚’æŠ½å‡º)
```

### ä¾‹3: ã‚«ã‚¹ã‚¿ãƒ ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰

```bash
python hybrid_scraping_tool.py
# 3ã‚’é¸æŠ
# URL: https://any-site.com

> find .product
> extract
ã‚³ãƒ³ãƒ†ãƒŠ: .product
ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å: name
nameã®ã‚»ãƒ¬ã‚¯ã‚¿: .product-name
ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å: price
priceã®ã‚»ãƒ¬ã‚¯ã‚¿: .price
ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å: (ç©ºã§Enter)
ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ y
```

---

## ğŸ¯ 3ã¤ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„åˆ†ã‘

### ãƒ„ãƒ¼ãƒ«1: `selenium_scraping_tool.py`
**ç”¨é€”:** ã‚µã‚¤ãƒˆæ§‹é€ ã®ç¢ºèª
- HTML/DOMæ§‹é€ ã‚’è‡ªå‹•åˆ†æ
- ãƒªãƒ³ã‚¯ãƒ»ç”»åƒã‚’ä¸€è¦§è¡¨ç¤º
- ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜

**ã“ã‚“ãªæ™‚ã«:**
ã€Œã“ã®ã‚µã‚¤ãƒˆã©ã‚“ãªæ§‹é€ ï¼Ÿã€

### ãƒ„ãƒ¼ãƒ«2: `html_parser_no_driver.py`
**ç”¨é€”:** é™çš„HTMLã®é«˜é€Ÿè§£æ
- ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦
- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ
- è¶…é«˜é€Ÿ

**ã“ã‚“ãªæ™‚ã«:**
ã€ŒHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ãŸã„ã€
ã€ŒJavaScriptä¸è¦ãªã‚µã‚¤ãƒˆã€

### ãƒ„ãƒ¼ãƒ«3: `hybrid_scraping_tool.py` â­**ãŠã™ã™ã‚**
**ç”¨é€”:** å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
- è‡ªå‹•åˆ¤å®šã§æœ€é©ãªæ–¹æ³•ã‚’é¸æŠ
- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºâ†’CSV/JSONä¿å­˜
- æœ¬ç•ªé‹ç”¨å‘ã‘

**ã“ã‚“ãªæ™‚ã«:**
ã€Œå®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ãŸã„ã€

---

## ğŸ“ ã‚ˆãã‚ã‚‹è³ªå•

### Q1: ã©ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ãˆã°ã„ã„ï¼Ÿ

**A:** è¿·ã£ãŸã‚‰`hybrid_scraping_tool.py`ã‚’ä½¿ã£ã¦ãã ã•ã„ï¼è‡ªå‹•ã§æœ€é©ãªæ–¹æ³•ã‚’é¸ã‚“ã§ãã‚Œã¾ã™ã€‚

### Q2: CSSã‚»ãƒ¬ã‚¯ã‚¿ãŒã‚ã‹ã‚‰ãªã„

**A:** 
1. ãƒ–ãƒ©ã‚¦ã‚¶ã§`F12`â†’é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«
2. å·¦ä¸Šã®çŸ¢å°ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
3. å–å¾—ã—ãŸã„è¦ç´ ã‚’ã‚¯ãƒªãƒƒã‚¯
4. å³ã‚¯ãƒªãƒƒã‚¯â†’Copyâ†’Copy selector

### Q3: JavaScriptã‚µã‚¤ãƒˆã‹ã©ã†ã‹åˆ¤æ–­ã§ããªã„

**A:** `hybrid_scraping_tool.py`ã®è‡ªå‹•åˆ¤å®šãƒ¢ãƒ¼ãƒ‰ã«ä»»ã›ã‚Œã°OKï¼

---



ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼

# ============================================
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«
# Selenium ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚° â†’ BeautifulSoup ã§è§£æ
# ä¸¡æ–¹ã®ãƒ¡ãƒªãƒƒãƒˆã‚’æ´»ã‹ã™ï¼
# ============================================

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import requests
import logging
import time
from datetime import datetime
from typing import List, Dict, Optional
import json
import csv


# ============================================
# ãƒ­ã‚°è¨­å®š
# ============================================

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hybrid_scraping.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


# ============================================
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
# ============================================

class HybridScraper:
    """
    Seleniumã¨BeautifulSoupã‚’çµ„ã¿åˆã‚ã›ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    JavaScriptå®Ÿè¡ŒãŒå¿…è¦ãªå ´åˆã¯Seleniumã€ä¸è¦ãªå ´åˆã¯requestsã‚’è‡ªå‹•åˆ¤å®š
    """
    
    def __init__(self, use_selenium: bool = None, headless: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            use_selenium: True=Seleniumå¼·åˆ¶, False=requestså¼·åˆ¶, None=è‡ªå‹•åˆ¤å®š
            headless: Seleniumã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
        """
        print("[DEBUG] HybridScraperã‚’åˆæœŸåŒ–")
        logger.info(f"HybridScraperåˆæœŸåŒ–: selenium={use_selenium}, headless={headless}")
        
        self.use_selenium = use_selenium  # None=è‡ªå‹•åˆ¤å®š
        self.headless = headless
        self.driver = None
        self.soup = None
        self.html = None
        self.url = None
        
        # requestsã®ãƒ˜ãƒƒãƒ€ãƒ¼
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                         'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        }
    
    def fetch(self, url: str, wait_time: int = 3) -> bool:
        """
        URLã‹ã‚‰HTMLã‚’å–å¾—ï¼ˆè‡ªå‹•åˆ¤å®šã¾ãŸã¯æŒ‡å®šæ–¹æ³•ï¼‰
        
        Args:
            url: å–å¾—ã™ã‚‹URL
            wait_time: Seleniumä½¿ç”¨æ™‚ã®å¾…æ©Ÿæ™‚é–“
            
        Returns:
            æˆåŠŸæ™‚True
        """
        self.url = url
        
        # è‡ªå‹•åˆ¤å®šãƒ¢ãƒ¼ãƒ‰
        if self.use_selenium is None:
            print("[DEBUG] ğŸ¤– å–å¾—æ–¹æ³•ã‚’è‡ªå‹•åˆ¤å®šä¸­...")
            logger.info("å–å¾—æ–¹æ³•è‡ªå‹•åˆ¤å®š")
            
            # ã¾ãšrequestsã§è©¦ã™ï¼ˆé«˜é€Ÿï¼‰
            if self._fetch_with_requests():
                print("[DEBUG] âœ… requests ã§å–å¾—æˆåŠŸï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰")
                logger.info("requestsä½¿ç”¨")
                
                # JavaScriptãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
                if self._needs_javascript():
                    print("[DEBUG] âš ï¸  JavaScriptãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã§ã™ã€‚Seleniumã§å†å–å¾—...")
                    logger.info("JavaScriptå¿…è¦ã¨åˆ¤å®šã€Seleniumã«åˆ‡ã‚Šæ›¿ãˆ")
                    return self._fetch_with_selenium(wait_time)
                else:
                    print("[DEBUG] âœ… é™çš„HTMLãƒšãƒ¼ã‚¸ã§ã™ï¼ˆrequestsã§ååˆ†ï¼‰")
                    logger.info("é™çš„HTMLã¨åˆ¤å®š")
                    return True
            else:
                # requestsã§å¤±æ•—ã—ãŸã‚‰Seleniumã‚’è©¦ã™
                print("[DEBUG] requestså¤±æ•—ã€Seleniumã§è©¦ã—ã¾ã™...")
                logger.info("requestså¤±æ•—ã€Seleniumã«åˆ‡ã‚Šæ›¿ãˆ")
                return self._fetch_with_selenium(wait_time)
        
        # Seleniumå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰
        elif self.use_selenium:
            print("[DEBUG] ğŸŒ Selenium ã§å–å¾—ï¼ˆæŒ‡å®šï¼‰")
            logger.info("Seleniumå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰")
            return self._fetch_with_selenium(wait_time)
        
        # requestså¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰
        else:
            print("[DEBUG] âš¡ requests ã§å–å¾—ï¼ˆæŒ‡å®šï¼‰")
            logger.info("requestså¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰")
            return self._fetch_with_requests()
    
    def _fetch_with_requests(self) -> bool:
        """requestsã§HTMLå–å¾—"""
        print(f"[DEBUG] requests: {self.url}")
        logger.debug(f"requestså–å¾—: {self.url}")
        
        try:
            response = requests.get(self.url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
            
            self.html = response.text
            self.soup = BeautifulSoup(self.html, 'html.parser')
            
            print(f"[DEBUG] âœ… HTMLå–å¾—: {len(self.html):,} æ–‡å­—")
            logger.info(f"requestsæˆåŠŸ: {len(self.html)}æ–‡å­—")
            
            return True
            
        except Exception as e:
            print(f"[DEBUG] âŒ requestså¤±æ•—: {e}")
            logger.error(f"requestså¤±æ•—: {e}")
            return False
    
    def _fetch_with_selenium(self, wait_time: int = 3) -> bool:
        """Seleniumã§HTMLå–å¾—"""
        print(f"[DEBUG] Selenium: {self.url}")
        logger.debug(f"Seleniumå–å¾—: {self.url}")
        
        try:
            # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•
            if not self.driver:
                self._start_driver()
            
            # URLã‚’é–‹ã
            self.driver.get(self.url)
            
            # å¾…æ©Ÿ
            print(f"[DEBUG] {wait_time}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(wait_time)
            
            # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®HTMLã‚’å–å¾—
            self.html = self.driver.page_source
            self.soup = BeautifulSoup(self.html, 'html.parser')
            
            print(f"[DEBUG] âœ… HTMLå–å¾—: {len(self.html):,} æ–‡å­—")
            logger.info(f"SeleniumæˆåŠŸ: {len(self.html)}æ–‡å­—")
            
            return True
            
        except Exception as e:
            print(f"[DEBUG] âŒ Seleniumå¤±æ•—: {e}")
            logger.error(f"Seleniumå¤±æ•—: {e}", exc_info=True)
            return False
    
    def _start_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•"""
        print("[DEBUG] Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•ä¸­...")
        logger.info("ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•")
        
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(10)
        
        print("[DEBUG] âœ… ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•å®Œäº†")
        logger.info("ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•å®Œäº†")
    
    def _needs_javascript(self) -> bool:
        """
        JavaScriptãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã‹åˆ¤å®š
        
        Returns:
            True=JSå¿…è¦, False=é™çš„HTML
        """
        if not self.soup:
            return False
        
        print("[DEBUG] JavaScriptå¿…è¦æ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # ãƒã‚§ãƒƒã‚¯1: bodyå†…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå°‘ãªã„
        body = self.soup.find('body')
        if body:
            body_text = body.get_text(strip=True)
            if len(body_text) < 100:  # bodyãŒç©ºã£ã½ã«è¿‘ã„
                print("[DEBUG]   â†’ bodyãŒç©ºï¼ˆJSã§ç”Ÿæˆã•ã‚Œã‚‹å¯èƒ½æ€§å¤§ï¼‰")
                logger.debug("bodyç©º: JSå¿…è¦ã¨åˆ¤å®š")
                return True
        
        # ãƒã‚§ãƒƒã‚¯2: React/Vue/Angularã®ç—•è·¡
        js_frameworks = [
            'react', 'vue', 'angular', 'next', 'nuxt',
            'data-react', 'ng-app', 'v-app', '__NEXT_DATA__'
        ]
        html_lower = self.html.lower()
        for framework in js_frameworks:
            if framework in html_lower:
                print(f"[DEBUG]   â†’ {framework}ã‚’æ¤œå‡ºï¼ˆJSãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰")
                logger.debug(f"JSãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¤œå‡º: {framework}")
                return True
        
        # ãƒã‚§ãƒƒã‚¯3: noscriptã‚¿ã‚°ã§è­¦å‘Š
        noscript = self.soup.find('noscript')
        if noscript and 'javascript' in noscript.get_text().lower():
            print("[DEBUG]   â†’ noscriptè­¦å‘Šã‚ã‚Š")
            logger.debug("noscriptè­¦å‘Š: JSå¿…è¦")
            return True
        
        print("[DEBUG]   â†’ é™çš„HTMLã¨åˆ¤å®š")
        logger.debug("é™çš„HTMLåˆ¤å®š")
        return False
    
    def find_all(self, selector: str, limit: int = None) -> List:
        """
        CSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            selector: CSSã‚»ãƒ¬ã‚¯ã‚¿
            limit: å–å¾—ã™ã‚‹æœ€å¤§æ•°
            
        Returns:
            è¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print(f"\n[DEBUG] CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢: '{selector}'")
        logger.info(f"CSSæ¤œç´¢: {selector}")
        
        elements = self.soup.select(selector, limit=limit)
        
        print(f"[DEBUG] âœ… {len(elements)}å€‹ç™ºè¦‹")
        logger.info(f"æ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
        for i, elem in enumerate(elements[:5], 1):
            text = elem.get_text(strip=True)[:50]
            print(f"[DEBUG]   [{i}] {text}...")
            logger.debug(f"è¦ç´ [{i}]: {text[:30]}")
        
        if len(elements) > 5:
            print(f"[DEBUG]   ... ä»– {len(elements) - 5}å€‹")
        
        return elements
    
    def extract_data(self, config: Dict) -> List[Dict]:
        """
        è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            config: æŠ½å‡ºè¨­å®šã®è¾æ›¸
                {
                    'container': 'CSSã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆè¦ªè¦ç´ ï¼‰',
                    'fields': {
                        'field_name': 'CSSã‚»ãƒ¬ã‚¯ã‚¿',
                        ...
                    }
                }
        
        Returns:
            æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print("\n[DEBUG] ========== ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹ ==========")
        logger.info("ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹")
        
        # ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‚’å–å¾—
        container_selector = config.get('container', 'body')
        containers = self.soup.select(container_selector)
        
        print(f"[DEBUG] ã‚³ãƒ³ãƒ†ãƒŠ: '{container_selector}'")
        print(f"[DEBUG] {len(containers)}å€‹ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ç™ºè¦‹")
        logger.info(f"ã‚³ãƒ³ãƒ†ãƒŠæ•°: {len(containers)}")
        
        results = []
        fields = config.get('fields', {})
        
        # å„ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        for i, container in enumerate(containers, 1):
            print(f"\n[DEBUG] --- ã‚³ãƒ³ãƒ†ãƒŠ [{i}] ---")
            
            item = {'_index': i}
            
            # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
            for field_name, field_selector in fields.items():
                try:
                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¦ç´ ã‚’æ¤œç´¢
                    element = container.select_one(field_selector)
                    
                    if element:
                        # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                        text = element.get_text(strip=True)
                        item[field_name] = text
                        print(f"[DEBUG]   {field_name}: {text[:50]}...")
                        logger.debug(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡º: {field_name}={text[:30]}")
                    else:
                        item[field_name] = None
                        print(f"[DEBUG]   {field_name}: (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")
                        logger.debug(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æœªç™ºè¦‹: {field_name}")
                
                except Exception as e:
                    item[field_name] = None
                    print(f"[DEBUG]   {field_name}: ã‚¨ãƒ©ãƒ¼ ({e})")
                    logger.warning(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {field_name}: {e}")
            
            results.append(item)
        
        print(f"\n[DEBUG] ========== æŠ½å‡ºå®Œäº†: {len(results)}ä»¶ ==========")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(results)}ä»¶")
        
        return results
    
    def save_to_csv(self, data: List[Dict], filename: str = None):
        """CSVã«ä¿å­˜"""
        if not data:
            print("[DEBUG] âš ï¸  ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'scraped_data_{timestamp}.csv'
        
        print(f"\n[DEBUG] CSVã«ä¿å­˜: {filename}")
        logger.info(f"CSVä¿å­˜: {filename}")
        
        try:
            with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=data[0].keys())
                writer.writeheader()
                writer.writerows(data)
            
            print(f"[DEBUG] âœ… {len(data)}ä»¶ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"CSVä¿å­˜å®Œäº†: {len(data)}ä»¶")
        
        except Exception as e:
            print(f"[DEBUG] âŒ CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_to_json(self, data: List[Dict], filename: str = None):
        """JSONã«ä¿å­˜"""
        if not data:
            print("[DEBUG] âš ï¸  ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'scraped_data_{timestamp}.json'
        
        print(f"\n[DEBUG] JSONã«ä¿å­˜: {filename}")
        logger.info(f"JSONä¿å­˜: {filename}")
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"[DEBUG] âœ… {len(data)}ä»¶ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"JSONä¿å­˜å®Œäº†: {len(data)}ä»¶")
        
        except Exception as e:
            print(f"[DEBUG] âŒ JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
        if self.driver:
            print("\n[DEBUG] ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†")
            logger.info("ãƒ‰ãƒ©ã‚¤ãƒãƒ¼çµ‚äº†")
            self.driver.quit()
            self.driver = None


# ============================================
# ä½¿ç”¨ä¾‹
# ============================================

def example_news_site():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹"""
    print("\n" + "=" * 70)
    print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹")
    print("=" * 70)
    
    scraper = HybridScraper(use_selenium=None)  # è‡ªå‹•åˆ¤å®š
    
    try:
        # URLã‚’å–å¾—
        url = input("\nãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®URL: ")
        
        if scraper.fetch(url):
            # æŠ½å‡ºè¨­å®š
            print("\nè¨˜äº‹ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
            container = input("è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠï¼ˆä¾‹: .article-itemï¼‰: ")
            title_sel = input("ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹: .titleï¼‰: ")
            link_sel = input("ãƒªãƒ³ã‚¯ï¼ˆä¾‹: aï¼‰: ")
            
            config = {
                'container': container,
                'fields': {
                    'title': title_sel,
                    'link': link_sel
                }
            }
            
            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            articles = scraper.extract_data(config)
            
            # ä¿å­˜
            if articles:
                scraper.save_to_csv(articles)
                scraper.save_to_json(articles)
    
    finally:
        scraper.close()


def example_product_site():
    """å•†å“ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹"""
    print("\n" + "=" * 70)
    print("ğŸ›ï¸ å•†å“ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹")
    print("=" * 70)
    
    scraper = HybridScraper(use_selenium=True, headless=False)  # Seleniumå¼·åˆ¶
    
    try:
        url = input("\nå•†å“ãƒšãƒ¼ã‚¸ã®URL: ")
        
        if scraper.fetch(url, wait_time=5):
            # æŠ½å‡ºè¨­å®š
            config = {
                'container': '.product-item',  # å•†å“ã‚³ãƒ³ãƒ†ãƒŠ
                'fields': {
                    'name': '.product-name',
                    'price': '.price',
                    'rating': '.rating'
                }
            }
            
            products = scraper.extract_data(config)
            
            if products:
                scraper.save_to_csv(products)
    
    finally:
        scraper.close()


# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ============================================

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 70)
    print("ğŸ”¥ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    print("=" * 70)
    logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹")
    
    print("\nå®Ÿè¡Œã™ã‚‹ä¾‹ã‚’é¸æŠ:")
    print("1. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆï¼ˆè‡ªå‹•åˆ¤å®šï¼‰")
    print("2. å•†å“ã‚µã‚¤ãƒˆï¼ˆSeleniumå¼·åˆ¶ï¼‰")
    print("3. ã‚«ã‚¹ã‚¿ãƒ è¨­å®š")
    
    choice = input("\né¸æŠ (1-3): ")
    
    if choice == "1":
        example_news_site()
    
    elif choice == "2":
        example_product_site()
    
    elif choice == "3":
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
        scraper = HybridScraper(use_selenium=None)
        
        try:
            url = input("\nURL: ")
            
            if scraper.fetch(url):
                # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
                print("\nã‚³ãƒãƒ³ãƒ‰:")
                print("  find <ã‚»ãƒ¬ã‚¯ã‚¿>  - è¦ç´ ã‚’æ¤œç´¢")
                print("  extract          - ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
                print("  quit             - çµ‚äº†")
                
                while True:
                    cmd = input("\n> ").strip()
                    
                    if cmd == "quit":
                        break
                    
                    elif cmd.startswith("find "):
                        selector = cmd[5:]
                        scraper.find_all(selector)
                    
                    elif cmd == "extract":
                        container = input("ã‚³ãƒ³ãƒ†ãƒŠã‚»ãƒ¬ã‚¯ã‚¿: ")
                        fields = {}
                        
                        while True:
                            field_name = input("ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åï¼ˆç©ºã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰: ")
                            if not field_name:
                                break
                            field_sel = input(f"{field_name}ã®ã‚»ãƒ¬ã‚¯ã‚¿: ")
                            fields[field_name] = field_sel
                        
                        config = {
                            'container': container,
                            'fields': fields
                        }
                        
                        data = scraper.extract_data(config)
                        
                        if data:
                            save = input("ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
                            if save.lower() == 'y':
                                scraper.save_to_csv(data)
                                scraper.save_to_json(data)
        
        finally:
            scraper.close()
    
    print("\n" + "=" * 70)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 70)
    logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")


if __name__ == "__main__":
    main()



ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
# ============================================
# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ HTMLè§£æãƒ„ãƒ¼ãƒ«
# BeautifulSoup + requests ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ã§è»½é‡ãƒ»é«˜é€Ÿ
# ============================================

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import requests  # HTTPé€šä¿¡ã§HTMLã‚’å–å¾—ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from bs4 import BeautifulSoup  # HTMLã‚’è§£æã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆBeautiful Soup 4ï¼‰
import logging  # ãƒ­ã‚°å‡ºåŠ›ç”¨ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from datetime import datetime  # æ—¥æ™‚ã‚’å–å¾—ãƒ»æ“ä½œã™ã‚‹ãŸã‚ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from typing import List, Dict, Optional  # å‹ãƒ’ãƒ³ãƒˆç”¨ï¼ˆã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§å‘ä¸Šï¼‰
from urllib.parse import urljoin, urlparse  # URLå‡¦ç†ç”¨ï¼ˆç›¸å¯¾URLâ†’çµ¶å¯¾URLå¤‰æ›ç­‰ï¼‰
import json  # JSONå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿
import csv  # CSVå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿


# ============================================
# ãƒ­ã‚°è¨­å®š
# ============================================

# ãƒ­ã‚®ãƒ³ã‚°ã®åŸºæœ¬è¨­å®šã‚’è¡Œã†ï¼ˆå…¨ä½“çš„ãªè¨­å®šï¼‰
logging.basicConfig(
    level=logging.DEBUG,  # DEBUGãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆæœ€ã‚‚è©³ç´°ãªãƒ¬ãƒ™ãƒ«ï¼‰
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # ãƒ­ã‚°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®šç¾©
    handlers=[  # ãƒ­ã‚°ã®å‡ºåŠ›å…ˆã‚’è¤‡æ•°æŒ‡å®šå¯èƒ½
        logging.FileHandler('html_analysis.log', encoding='utf-8'),  # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        logging.StreamHandler()  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼ˆã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼‰ã«ã‚‚è¡¨ç¤º
    ]
)

# ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
logger = logging.getLogger(__name__)  # __name__ã¯ç¾åœ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’è‡ªå‹•å–å¾—


# ============================================
# HTMLè§£æã‚¯ãƒ©ã‚¹
# ============================================

class HTMLAnalyzer:
    """
    ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ã§HTMLã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹
    requests + BeautifulSoup ã‚’ä½¿ç”¨ã—ã¦è»½é‡ãƒ»é«˜é€Ÿã«å‹•ä½œ
    """
    
    def __init__(self):
        """
        ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ï¼‰
        ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆæ™‚ã«è‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã‚‹ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰
        """
        # ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆã§åˆæœŸåŒ–é–‹å§‹ã‚’é€šçŸ¥
        print("[DEBUG] HTMLAnalyzerã‚’åˆæœŸåŒ–")
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        logger.info("HTMLAnalyzeråˆæœŸåŒ–")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¾æ›¸ã§å®šç¾©ï¼ˆãƒœãƒƒãƒˆæ¤œå‡ºã‚’å›é¿ï¼‰
        self.headers = {
            # User-Agent: ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å½è£…ï¼ˆé€šå¸¸ã®ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã«è¦‹ã›ã‹ã‘ã‚‹ï¼‰
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                         '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            # Accept: å—ã‘å…¥ã‚Œå¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            # Accept-Language: å„ªå…ˆã™ã‚‹è¨€èªã‚’æŒ‡å®šï¼ˆæ—¥æœ¬èªå„ªå…ˆï¼‰
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            # Accept-Encoding: å—ã‘å…¥ã‚Œå¯èƒ½ãªåœ§ç¸®å½¢å¼ã‚’æŒ‡å®š
            'Accept-Encoding': 'gzip, deflate, br',
            # Connection: æ¥ç¶šæ–¹å¼ã‚’æŒ‡å®š
            'Connection': 'keep-alive',
        }
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã‚’åˆæœŸåŒ–ï¼ˆå…¨ã¦Noneã‹ã‚‰å§‹ã‚ã‚‹ï¼‰
        self.soup = None  # BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆHTMLãƒ‘ãƒ¼ã‚¹å¾Œï¼‰
        self.url = None  # ç¾åœ¨ã®URL
        self.html = None  # å–å¾—ã—ãŸHTMLæ–‡å­—åˆ—
    
    def fetch_url(self, url: str, timeout: int = 10) -> bool:
        """
        URLã‹ã‚‰HTMLã‚’å–å¾—
        HTTPé€šä¿¡ã§Webãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—ã™ã‚‹
        
        Args:
            url: å–å¾—ã™ã‚‹URLï¼ˆæ–‡å­—åˆ—ï¼‰
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰å¿œç­”ãŒãªã„å ´åˆã®å¾…æ©Ÿä¸Šé™
            
        Returns:
            æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚Falseï¼ˆboolå‹ï¼‰
        """
        # URLã‚¢ã‚¯ã‚»ã‚¹é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] URLã«ã‚¢ã‚¯ã‚»ã‚¹: {url}")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"URLå–å¾—é–‹å§‹: {url}")
        
        try:  # ä¾‹å¤–å‡¦ç†ã®tryãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            # requestsã§HTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
            response = requests.get(
                url,  # å–å¾—ã™ã‚‹URL
                headers=self.headers,  # è¨­å®šã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼
                timeout=timeout  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“
            )
            
            # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
            print(f"[DEBUG] ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            # ãƒ­ã‚°ã«ã‚‚è¨˜éŒ²
            logger.debug(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ4xx,5xxãªã‚‰HTTPErrorã‚’ç™ºç”Ÿï¼‰
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
            if response.encoding == 'ISO-8859-1':  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆ
                # è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å¤‰æ›´
                response.encoding = response.apparent_encoding
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰HTMLæ–‡å­—åˆ—ã‚’å–å¾—ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜
            self.html = response.text
            # URLã‚‚ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜
            self.url = url
            
            # BeautifulSoupã§HTMLã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè§£æï¼‰
            self.soup = BeautifulSoup(self.html, 'html.parser')  # html.parserã‚’ä½¿ç”¨
            
            # å–å¾—æˆåŠŸã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
            print(f"[DEBUG] âœ… HTMLå–å¾—æˆåŠŸ")
            # HTMLé•·ã‚’è¡¨ç¤ºï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§èª­ã¿ã‚„ã™ãï¼‰
            print(f"[DEBUG] HTMLé•·: {len(self.html):,} æ–‡å­—")
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡¨ç¤º
            print(f"[DEBUG] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
            
            # ãƒ­ã‚°ã«æˆåŠŸã‚’è¨˜éŒ²
            logger.info(f"HTMLå–å¾—æˆåŠŸ: {len(self.html)}æ–‡å­—")
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚‚ãƒ­ã‚°ã«è¨˜éŒ²
            logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
            
            return True  # æˆåŠŸã‚’ç¤ºã™Trueã‚’è¿”ã™
            
        except requests.exceptions.Timeout:  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’ä»¥å†…ã«å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return False  # å¤±æ•—ã‚’ç¤ºã™Falseã‚’è¿”ã™
            
        except requests.exceptions.HTTPError as e:  # HTTPã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ HTTPã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            logger.error(f"HTTPã‚¨ãƒ©ãƒ¼: {e}")
            return False  # å¤±æ•—ã‚’è¿”ã™
            
        except requests.exceptions.RequestException as e:  # ãã®ä»–ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ï¼ˆexc_info=Trueã§ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚‚è¨˜éŒ²ï¼‰
            logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False  # å¤±æ•—ã‚’è¿”ã™
    
    def load_from_file(self, filepath: str) -> bool:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        ä¿å­˜æ¸ˆã¿ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã™ã‚‹å ´åˆã«ä½¿ç”¨
        
        Args:
            filepath: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
            
        Returns:
            æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {filepath}")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {filepath}")
        
        try:  # ä¾‹å¤–å‡¦ç†ã®tryãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§é–‹ãï¼ˆUTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
            with open(filepath, 'r', encoding='utf-8') as f:
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å…¨ã¦èª­ã¿è¾¼ã‚“ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜
                self.html = f.read()
            
            # BeautifulSoupã§HTMLã‚’ãƒ‘ãƒ¼ã‚¹
            self.soup = BeautifulSoup(self.html, 'html.parser')
            # URLã¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆfile://ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼‰
            self.url = f"file://{filepath}"
            
            # èª­ã¿è¾¼ã¿æˆåŠŸã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
            print(f"[DEBUG] âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            # HTMLé•·ã‚’è¡¨ç¤º
            print(f"[DEBUG] HTMLé•·: {len(self.html):,} æ–‡å­—")
            
            # ãƒ­ã‚°ã«æˆåŠŸã‚’è¨˜éŒ²
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(self.html)}æ–‡å­—")
            
            return True  # æˆåŠŸã‚’è¿”ã™
            
        except FileNotFoundError:  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {filepath}")
            return False  # å¤±æ•—ã‚’è¿”ã™
            
        except Exception as e:  # ãã®ä»–ã®ä¾‹å¤–
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ä»˜ãï¼‰
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False  # å¤±æ•—ã‚’è¿”ã™
    
    def get_page_info(self) -> Dict[str, any]:
        """
        ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        ã‚¿ã‚¤ãƒˆãƒ«ã€ãƒ¡ã‚¿æƒ…å ±ãªã©ã‚’æŠ½å‡º
        
        Returns:
            ãƒšãƒ¼ã‚¸æƒ…å ±ã®è¾æ›¸
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆï¼ˆHTMLæœªèª­ã¿è¾¼ã¿ï¼‰
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            # ãƒ­ã‚°ã«è­¦å‘Šã‚’è¨˜éŒ²
            logger.warning("HTMLæœªèª­ã¿è¾¼ã¿")
            return {}  # ç©ºã®è¾æ›¸ã‚’è¿”ã™
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ========== ãƒšãƒ¼ã‚¸åŸºæœ¬æƒ…å ± ==========")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("ãƒšãƒ¼ã‚¸æƒ…å ±å–å¾—é–‹å§‹")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯"(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"ï¼‰
        title = self.soup.title.string if self.soup.title else "(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"
        
        # ãƒ¡ã‚¿æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹å¤‰æ•°ã‚’åˆæœŸåŒ–
        description = ""  # descriptionï¼ˆãƒšãƒ¼ã‚¸ã®èª¬æ˜ï¼‰
        keywords = ""  # keywordsï¼ˆãƒšãƒ¼ã‚¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        
        # descriptionãƒ¡ã‚¿ã‚¿ã‚°ã‚’æ¤œç´¢
        meta_desc = self.soup.find('meta', attrs={'name': 'description'})
        # ãƒ¡ã‚¿ã‚¿ã‚°ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        if meta_desc:
            # contentå±æ€§ã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
            description = meta_desc.get('content', '')
        
        # keywordsãƒ¡ã‚¿ã‚¿ã‚°ã‚’æ¤œç´¢
        meta_keywords = self.soup.find('meta', attrs={'name': 'keywords'})
        # ãƒ¡ã‚¿ã‚¿ã‚°ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        if meta_keywords:
            # contentå±æ€§ã‚’å–å¾—
            keywords = meta_keywords.get('content', '')
        
        # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
        info = {
            'url': self.url,  # URL
            'title': title,  # ã‚¿ã‚¤ãƒˆãƒ«
            'description': description,  # èª¬æ˜
            'keywords': keywords,  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'html_length': len(self.html)  # HTMLé•·
        }
        
        # æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"[DEBUG] URL: {info['url']}")
        print(f"[DEBUG] ã‚¿ã‚¤ãƒˆãƒ«: {info['title']}")
        # descriptionã¯é•·ã„å ´åˆãŒã‚ã‚‹ã®ã§æœ€åˆã®100æ–‡å­—ã®ã¿è¡¨ç¤º
        print(f"[DEBUG] èª¬æ˜: {info['description'][:100] if info['description'] else '(ãªã—)'}...")
        # keywordsã‚‚åŒæ§˜
        print(f"[DEBUG] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {info['keywords'][:100] if info['keywords'] else '(ãªã—)'}...")
        # HTMLé•·ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¡¨ç¤º
        print(f"[DEBUG] HTMLé•·: {info['html_length']:,} æ–‡å­—")
        
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ãƒšãƒ¼ã‚¸æƒ…å ±: {title}")
        logger.debug(f"HTMLé•·: {info['html_length']}")
        
        # æƒ…å ±ã®è¾æ›¸ã‚’è¿”ã™
        return info
    
    def analyze_structure(self) -> Dict[str, int]:
        """
        HTMLæ§‹é€ ã‚’åˆ†æï¼ˆè¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼‰
        å„HTMLè¦ç´ ãŒã„ãã¤å­˜åœ¨ã™ã‚‹ã‹ã‚’èª¿ã¹ã‚‹
        
        Returns:
            è¦ç´ æ•°ã®è¾æ›¸ï¼ˆã‚­ãƒ¼=è¦ç´ åã€å€¤=å€‹æ•°ï¼‰
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}  # ç©ºã®è¾æ›¸ã‚’è¿”ã™
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ========== HTMLæ§‹é€ åˆ†æ ==========")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("æ§‹é€ åˆ†æé–‹å§‹")
        
        # ã‚«ã‚¦ãƒ³ãƒˆå¯¾è±¡ã®ä¸»è¦ãªè¦ç´ ã‚’ãƒªã‚¹ãƒˆã§å®šç¾©
        elements = [
            'div', 'span', 'p', 'a', 'img', 'table', 'tr', 'td',  # åŸºæœ¬è¦ç´ 
            'ul', 'ol', 'li',  # ãƒªã‚¹ãƒˆè¦ç´ 
            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',  # è¦‹å‡ºã—è¦ç´ 
            'form', 'input', 'button', 'select', 'textarea',  # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ 
            'nav', 'header', 'footer', 'section', 'article', 'aside'  # HTML5ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´ 
        ]
        
        # ã‚«ã‚¦ãƒ³ãƒˆçµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
        counts = {}
        
        # ã‚«ã‚¦ãƒ³ãƒˆé–‹å§‹ã‚’é€šçŸ¥
        print("[DEBUG] è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆä¸­...")
        
        # å„è¦ç´ ã«ã¤ã„ã¦ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for element in elements:
            # è¦ç´ ã‚’å…¨ã¦æ¤œç´¢ï¼ˆfind_all=å…¨ã¦è¦‹ã¤ã‘ã‚‹ï¼‰
            found = self.soup.find_all(element)
            # è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®æ•°ã‚’å–å¾—
            count = len(found)
            # è¾æ›¸ã«è¦ç´ åã‚’ã‚­ãƒ¼ã€å€‹æ•°ã‚’å€¤ã¨ã—ã¦ä¿å­˜
            counts[element] = count
            
            # 1å€‹ä»¥ä¸Šå­˜åœ¨ã™ã‚‹è¦ç´ ã®ã¿è¡¨ç¤º
            if count > 0:
                # è¦ç´ åã¨å€‹æ•°ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
                print(f"[DEBUG]   <{element}>: {count}å€‹")
                # ãƒ­ã‚°ã«ã‚‚è¨˜éŒ²
                logger.debug(f"è¦ç´ : <{element}> = {count}")
        
        # ãƒ­ã‚°ã«å®Œäº†ã‚’è¨˜éŒ²
        logger.info(f"æ§‹é€ åˆ†æå®Œäº†: {len(counts)}ç¨®é¡ã®è¦ç´ ")
        
        # ã‚«ã‚¦ãƒ³ãƒˆçµæœã®è¾æ›¸ã‚’è¿”ã™
        return counts
    
    def find_by_class(self, class_name: str) -> List:
        """
        ã‚¯ãƒ©ã‚¹åã§è¦ç´ ã‚’æ¤œç´¢
        HTMLã®classå±æ€§ã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            class_name: æ¤œç´¢ã™ã‚‹ã‚¯ãƒ©ã‚¹åï¼ˆæ–‡å­—åˆ—ï¼‰
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []  # ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        # æ¤œç´¢é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢: '{class_name}'")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ã‚¯ãƒ©ã‚¹æ¤œç´¢: {class_name}")
        
        # ã‚¯ãƒ©ã‚¹åã§è¦ç´ ã‚’å…¨ã¦æ¤œç´¢ï¼ˆclass_=ã§classå±æ€§ã‚’æŒ‡å®šï¼‰
        elements = self.soup.find_all(class_=class_name)
        
        # æ¤œç´¢çµæœã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ã‚¯ãƒ©ã‚¹æ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # æœ€åˆã®5å€‹ã®è¦ç´ æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼‰
        for i, element in enumerate(elements[:5], 1):  # enumerate=ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ããƒ«ãƒ¼ãƒ—
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆstrip=å‰å¾Œã®ç©ºç™½å‰Šé™¤ã€[:50]=æœ€åˆã®50æ–‡å­—ï¼‰
            text = element.get_text(strip=True)[:50]
            # è¦ç´ ã®ã‚¿ã‚°åã‚’å–å¾—
            tag = element.name
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¿ã‚°åã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            print(f"[DEBUG]   [{i}] <{tag}> {text}...")
            # ãƒ­ã‚°ã«ã‚‚è¨˜éŒ²ï¼ˆæœ€åˆã®30æ–‡å­—ã®ã¿ï¼‰
            logger.debug(f"è¦ç´ [{i}]: <{tag}> {text[:30]}")
        
        # 5å€‹ã‚ˆã‚Šå¤šã„å ´åˆ
        if len(elements) > 5:
            # æ®‹ã‚Šã®å€‹æ•°ã‚’è¡¨ç¤º
            print(f"[DEBUG]   ... ä»– {len(elements) - 5}å€‹")
        
        # è¦ç´ ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return elements
    
    def find_by_id(self, element_id: str):
        """
        IDã§è¦ç´ ã‚’æ¤œç´¢
        HTMLã®idå±æ€§ã§è¦ç´ ã‚’æ¤œç´¢ï¼ˆIDã¯ä¸€æ„ãªã®ã§1ã¤ã®ã¿ï¼‰
        
        Args:
            element_id: æ¤œç´¢ã™ã‚‹è¦ç´ ã®IDï¼ˆæ–‡å­—åˆ—ï¼‰
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã¾ãŸã¯None
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return None  # Noneã‚’è¿”ã™
        
        # æ¤œç´¢é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] IDã§æ¤œç´¢: '{element_id}'")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"IDæ¤œç´¢: {element_id}")
        
        # IDã§è¦ç´ ã‚’æ¤œç´¢ï¼ˆfind=æœ€åˆã®1ã¤ã®ã¿ã€id=ã§idå±æ€§ã‚’æŒ‡å®šï¼‰
        element = self.soup.find(id=element_id)
        
        # è¦ç´ ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
        if element:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆæœ€åˆã®100æ–‡å­—ã®ã¿ï¼‰
            text = element.get_text(strip=True)[:100]
            # ã‚¿ã‚°åã‚’å–å¾—
            tag = element.name
            # æˆåŠŸã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
            print(f"[DEBUG] âœ… è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            # ã‚¿ã‚°åã‚’è¡¨ç¤º
            print(f"[DEBUG]   ã‚¿ã‚°: <{tag}>")
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            print(f"[DEBUG]   ãƒ†ã‚­ã‚¹ãƒˆ: {text}...")
            # ãƒ­ã‚°ã«æˆåŠŸã‚’è¨˜éŒ²
            logger.info(f"IDæ¤œç´¢æˆåŠŸ: {element_id}")
        else:  # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            # ãƒ­ã‚°ã«è­¦å‘Šã‚’è¨˜éŒ²
            logger.warning(f"IDæ¤œç´¢å¤±æ•—: {element_id}")
        
        # è¦ç´ ã‚’è¿”ã™ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
        return element
    
    def find_by_tag(self, tag_name: str) -> List:
        """
        ã‚¿ã‚°åã§è¦ç´ ã‚’æ¤œç´¢
        ç‰¹å®šã®HTMLã‚¿ã‚°ã‚’å…¨ã¦æ¤œç´¢
        
        Args:
            tag_name: æ¤œç´¢ã™ã‚‹ã‚¿ã‚°åï¼ˆä¾‹: 'div', 'p', 'a'ï¼‰
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []  # ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        # æ¤œç´¢é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] ã‚¿ã‚°åã§æ¤œç´¢: '<{tag_name}>'")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ã‚¿ã‚°æ¤œç´¢: {tag_name}")
        
        # ã‚¿ã‚°åã§è¦ç´ ã‚’å…¨ã¦æ¤œç´¢
        elements = self.soup.find_all(tag_name)
        
        # æ¤œç´¢çµæœã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ã‚¿ã‚°æ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # è¦ç´ ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return elements
    
    def find_by_css_selector(self, selector: str) -> List:
        """
        CSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢
        è¤‡é›‘ãªæ¤œç´¢æ¡ä»¶ã§ã‚‚å¯¾å¿œå¯èƒ½ï¼ˆæœ€ã‚‚æŸ”è»Ÿãªæ¤œç´¢æ–¹æ³•ï¼‰
        
        Args:
            selector: CSSã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆä¾‹: 'div.class', '#id', 'div > p'ï¼‰
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []  # ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        # æ¤œç´¢é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢: '{selector}'")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"CSSæ¤œç´¢: {selector}")
        
        # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢ï¼ˆselect=CSSã‚»ãƒ¬ã‚¯ã‚¿ä½¿ç”¨ï¼‰
        elements = self.soup.select(selector)
        
        # æ¤œç´¢çµæœã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"CSSæ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # æœ€åˆã®5å€‹ã®è¦ç´ æƒ…å ±ã‚’è¡¨ç¤º
        for i, element in enumerate(elements[:5], 1):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆæœ€åˆã®50æ–‡å­—ã®ã¿ï¼‰
            text = element.get_text(strip=True)[:50]
            # ã‚¿ã‚°åã‚’å–å¾—
            tag = element.name
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¿ã‚°ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            print(f"[DEBUG]   [{i}] <{tag}> {text}...")
            # ãƒ­ã‚°ã«ã‚‚è¨˜éŒ²ï¼ˆæœ€åˆã®30æ–‡å­—ã®ã¿ï¼‰
            logger.debug(f"è¦ç´ [{i}]: <{tag}> {text[:30]}")
        
        # 5å€‹ã‚ˆã‚Šå¤šã„å ´åˆ
        if len(elements) > 5:
            # æ®‹ã‚Šã®å€‹æ•°ã‚’è¡¨ç¤º
            print(f"[DEBUG]   ... ä»– {len(elements) - 5}å€‹")
        
        # è¦ç´ ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return elements
    
    def get_all_links(self) -> List[Dict[str, str]]:
        """
        ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        ãƒšãƒ¼ã‚¸å†…ã®<a>ã‚¿ã‚°ã‚’å…¨ã¦æŠ½å‡º
        
        Returns:
            ãƒªãƒ³ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆè¾æ›¸ã®ãƒªã‚¹ãƒˆï¼‰
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []  # ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ========== ãƒªãƒ³ã‚¯ä¸€è¦§ ==========")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("ãƒªãƒ³ã‚¯å–å¾—é–‹å§‹")
        
        # ã™ã¹ã¦ã®<a>ã‚¿ã‚°ã‚’æ¤œç´¢
        links = self.soup.find_all('a')
        
        # ãƒªãƒ³ã‚¯æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
        link_data = []
        
        # è¦‹ã¤ã‹ã£ãŸãƒªãƒ³ã‚¯æ•°ã‚’è¡¨ç¤º
        print(f"[DEBUG] {len(links)}å€‹ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        
        # å„ãƒªãƒ³ã‚¯ã®æƒ…å ±ã‚’å–å¾—ï¼ˆæœ€åˆã®10å€‹ã®ã¿å‡¦ç†ï¼‰
        for i, link in enumerate(links[:10], 1):
            # hrefå±æ€§ã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
            href = link.get('href', '')
            # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆå‰å¾Œã®ç©ºç™½å‰Šé™¤ï¼‰
            text = link.get_text(strip=True)
            
            # hrefãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
            if href:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›ï¼ˆurljoinä½¿ç”¨ï¼‰
                absolute_url = urljoin(self.url, href) if self.url else href
                
                # ãƒªãƒ³ã‚¯æƒ…å ±ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
                link_info = {
                    'href': absolute_url,  # çµ¶å¯¾URL
                    'text': text if text else '(ãƒ†ã‚­ã‚¹ãƒˆãªã—)',  # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
                    'original_href': href  # å…ƒã®hrefï¼ˆç›¸å¯¾URLã®å ´åˆã‚‚ã‚ã‚‹ï¼‰
                }
                # ãƒªã‚¹ãƒˆã«è¿½åŠ 
                link_data.append(link_info)
                
                # ãƒªãƒ³ã‚¯æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆï¼ˆæœ€åˆã®40æ–‡å­—ã®ã¿ï¼‰
                print(f"[DEBUG]   [{i}] {link_info['text'][:40]}")
                # URLã‚’è¡¨ç¤º
                print(f"[DEBUG]       â†’ {absolute_url}")
                # ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆæœ€åˆã®30æ–‡å­—ã®ã¿ï¼‰
                logger.debug(f"ãƒªãƒ³ã‚¯[{i}]: {text[:30]} -> {href}")
        
        # 10å€‹ã‚ˆã‚Šå¤šã„å ´åˆ
        if len(links) > 10:
            # æ®‹ã‚Šã®å€‹æ•°ã‚’è¡¨ç¤º
            print(f"[DEBUG]   ... ä»– {len(links) - 10}å€‹")
        
        # ãƒ­ã‚°ã«å®Œäº†ã‚’è¨˜éŒ²
        logger.info(f"ãƒªãƒ³ã‚¯å–å¾—å®Œäº†: {len(link_data)}å€‹")
        
        # ãƒªãƒ³ã‚¯æƒ…å ±ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return link_data
    
    def get_all_images(self) -> List[Dict[str, str]]:
        """
        ã™ã¹ã¦ã®ç”»åƒã‚’å–å¾—
        ãƒšãƒ¼ã‚¸å†…ã®<img>ã‚¿ã‚°ã‚’å…¨ã¦æŠ½å‡º
        
        Returns:
            ç”»åƒæƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆè¾æ›¸ã®ãƒªã‚¹ãƒˆï¼‰
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []  # ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ========== ç”»åƒä¸€è¦§ ==========")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("ç”»åƒå–å¾—é–‹å§‹")
        
        # ã™ã¹ã¦ã®<img>ã‚¿ã‚°ã‚’æ¤œç´¢
        images = self.soup.find_all('img')
        
        # ç”»åƒæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
        image_data = []
        
        # è¦‹ã¤ã‹ã£ãŸç”»åƒæ•°ã‚’è¡¨ç¤º
        print(f"[DEBUG] {len(images)}å€‹ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ç”»åƒæ•°: {len(images)}")
        
        # å„ç”»åƒã®æƒ…å ±ã‚’å–å¾—ï¼ˆæœ€åˆã®10å€‹ã®ã¿å‡¦ç†ï¼‰
        for i, img in enumerate(images[:10], 1):
            # srcå±æ€§ã‚’å–å¾—ï¼ˆç”»åƒã®URLï¼‰
            src = img.get('src', '')
            # altå±æ€§ã‚’å–å¾—ï¼ˆç”»åƒã®èª¬æ˜ï¼‰
            alt = img.get('alt', '')
            
            # srcãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
            if src:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                absolute_url = urljoin(self.url, src) if self.url else src
                
                # ç”»åƒæƒ…å ±ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
                img_info = {
                    'src': absolute_url,  # çµ¶å¯¾URL
                    'alt': alt if alt else '(altãªã—)',  # altå±æ€§
                    'original_src': src  # å…ƒã®src
                }
                # ãƒªã‚¹ãƒˆã«è¿½åŠ 
                image_data.append(img_info)
                
                # ç”»åƒæƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆï¼ˆæœ€åˆã®40æ–‡å­—ã®ã¿ï¼‰
                print(f"[DEBUG]   [{i}] alt='{img_info['alt'][:40]}'")
                # srcã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®60æ–‡å­—ã®ã¿ï¼‰
                print(f"[DEBUG]       src: {absolute_url[:60]}...")
                # ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆæœ€åˆã®50æ–‡å­—ã®ã¿ï¼‰
                logger.debug(f"ç”»åƒ[{i}]: alt={alt} -> {src[:50]}")
        
        # 10å€‹ã‚ˆã‚Šå¤šã„å ´åˆ
        if len(images) > 10:
            # æ®‹ã‚Šã®å€‹æ•°ã‚’è¡¨ç¤º
            print(f"[DEBUG]   ... ä»– {len(images) - 10}å€‹")
        
        # ãƒ­ã‚°ã«å®Œäº†ã‚’è¨˜éŒ²
        logger.info(f"ç”»åƒå–å¾—å®Œäº†: {len(image_data)}å€‹")
        
        # ç”»åƒæƒ…å ±ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return image_data
    
    def save_html(self, filename: Optional[str] = None):
        """
        HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        å–å¾—ã—ãŸHTMLã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            filename: ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        # HTMLãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.html:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return  # å‡¦ç†ã‚’çµ‚äº†
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
        if not filename:
            # ç¾åœ¨æ—¥æ™‚ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è‡ªå‹•ç”Ÿæˆ
            filename = f'saved_html_{timestamp}.html'
        
        # ä¿å­˜é–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print(f"\n[DEBUG] HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {filename}")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"HTMLä¿å­˜: {filename}")
        
        try:  # ä¾‹å¤–å‡¦ç†ã®tryãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã§é–‹ãï¼ˆUTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
            with open(filename, 'w', encoding='utf-8') as f:
                # HTMLæ–‡å­—åˆ—ã‚’æ›¸ãè¾¼ã‚€
                f.write(self.html)
            
            # ä¿å­˜æˆåŠŸã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æ–‡å­—æ•°è¡¨ç¤ºï¼‰
            print(f"[DEBUG] âœ… ä¿å­˜æˆåŠŸ: {len(self.html):,} æ–‡å­—")
            # ãƒ­ã‚°ã«æˆåŠŸã‚’è¨˜éŒ²
            logger.info(f"HTMLä¿å­˜å®Œäº†: {len(self.html)}æ–‡å­—")
            
        except Exception as e:  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print(f"[DEBUG] âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            logger.error(f"HTMLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_text(self) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
        HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å–å¾—
        
        Returns:
            ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return ""  # ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
        
        # æŠ½å‡ºé–‹å§‹ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­...")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å–å¾—ï¼ˆseparator=æ”¹è¡Œã§åŒºåˆ‡ã‚‹ã€strip=ç©ºç™½å‰Šé™¤ï¼‰
        text = self.soup.get_text(separator='\n', strip=True)
        
        # æŠ½å‡ºå®Œäº†ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æ–‡å­—æ•°è¡¨ç¤ºï¼‰
        print(f"[DEBUG] âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text):,} æ–‡å­—")
        # ãƒ­ã‚°ã«å®Œäº†ã‚’è¨˜éŒ²
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text)}æ–‡å­—")
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è¿”ã™
        return text
    
    def pretty_print(self):
        """
        HTMLã‚’æ•´å½¢ã—ã¦è¡¨ç¤º
        ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä»˜ã‘ã¦èª­ã¿ã‚„ã™ãæ•´å½¢
        """
        # soupãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if not self.soup:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return  # å‡¦ç†ã‚’çµ‚äº†
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        print("\n[DEBUG] ========== æ•´å½¢HTML ==========")
        
        # prettify()ã§æ•´å½¢ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãHTMLï¼‰
        pretty_html = self.soup.prettify()
        
        # æœ€åˆã®50è¡Œã®ã¿è¡¨ç¤ºï¼ˆæ”¹è¡Œã§åˆ†å‰²ã—ã¦ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
        lines = pretty_html.split('\n')[:50]
        # æ”¹è¡Œã§çµåˆã—ã¦è¡¨ç¤º
        print('\n'.join(lines))
        
        # 50è¡Œã‚ˆã‚Šå¤šã„å ´åˆ
        if len(pretty_html.split('\n')) > 50:
            # æ®‹ã‚Šã®è¡Œæ•°ã‚’è¡¨ç¤º
            print(f"\n... ä»– {len(pretty_html.split('\n')) - 50}è¡Œ")


# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ============================================

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    # ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
    print("=" * 70)
    print("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ HTMLè§£æãƒ„ãƒ¼ãƒ«")
    print("=" * 70)
    # ãƒ­ã‚°ã«é–‹å§‹ã‚’è¨˜éŒ²
    logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹")
    
    # HTMLAnalyzerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    analyzer = HTMLAnalyzer()
    
    # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º
    print("\nè§£ææ–¹æ³•ã‚’é¸æŠ:")
    print("1. URLã‹ã‚‰å–å¾—")
    print("2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠã‚’å–å¾—
    choice = input("\né¸æŠ (1-2): ")
    
    try:  # ä¾‹å¤–å‡¦ç†ã®tryãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
        # é¸æŠ1ã®å ´åˆï¼ˆURLã‹ã‚‰å–å¾—ï¼‰
        if choice == "1":
            # URLã‚’å…¥åŠ›
            url = input("URL: ")
            # URLã‹ã‚‰HTMLå–å¾—ã‚’è©¦ã¿ã‚‹
            if not analyzer.fetch_url(url):
                # å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                print("[DEBUG] âŒ HTMLå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return  # å‡¦ç†ã‚’çµ‚äº†
        
        # é¸æŠ2ã®å ´åˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        elif choice == "2":
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å…¥åŠ›
            filepath = input("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹: ")
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
            if not analyzer.load_from_file(filepath):
                # å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                print("[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return  # å‡¦ç†ã‚’çµ‚äº†
        
        # ãã‚Œä»¥å¤–ã®é¸æŠã®å ´åˆ
        else:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            print("[DEBUG] âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return  # å‡¦ç†ã‚’çµ‚äº†
        
        # --- ã“ã“ã‹ã‚‰è‡ªå‹•åˆ†æé–‹å§‹ ---
        
        # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        page_info = analyzer.get_page_info()
        
        # HTMLæ§‹é€ ã‚’åˆ†æã—ã¦è¡¨ç¤º
        structure = analyzer.analyze_structure()
        
        # ãƒªãƒ³ã‚¯ä¸€è¦§ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        links = analyzer.get_all_links()
        
        # ç”»åƒä¸€è¦§ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        images = analyzer.get_all_images()
        
        # HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        analyzer.save_html()
        
        # --- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ ---
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
        print("\n" + "=" * 70)
        print("ğŸ“‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("=" * 70)
        # ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§ã‚’è¡¨ç¤º
        print("ã‚³ãƒãƒ³ãƒ‰:")
        print("  class <ã‚¯ãƒ©ã‚¹å>  - ã‚¯ãƒ©ã‚¹ã§æ¤œç´¢")
        print("  id <ID>          - IDã§æ¤œç´¢")
        print("  tag <ã‚¿ã‚°å>      - ã‚¿ã‚°ã§æ¤œç´¢")
        print("  css <ã‚»ãƒ¬ã‚¯ã‚¿>    - CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢")
        print("  text             - ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º")
        print("  quit             - çµ‚äº†")
        
        # ç„¡é™ãƒ«ãƒ¼ãƒ—ã§ã‚³ãƒãƒ³ãƒ‰å…¥åŠ›ã‚’å¾…ã¤
        while True:
            # ã‚³ãƒãƒ³ãƒ‰å…¥åŠ›ï¼ˆå‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤ï¼‰
            command = input("\n> ").strip()
            
            # quitã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            if command == "quit":
                # ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break
            
            # classã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            elif command.startswith("class "):
                # "class "ä»¥é™ã‚’ã‚¯ãƒ©ã‚¹åã¨ã—ã¦å–å¾—
                class_name = command[6:].strip()
                # ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢
                analyzer.find_by_class(class_name)
            
            # idã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            elif command.startswith("id "):
                # "id "ä»¥é™ã‚’IDã¨ã—ã¦å–å¾—
                element_id = command[3:].strip()
                # IDã§æ¤œç´¢
                analyzer.find_by_id(element_id)
            
            # tagã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            elif command.startswith("tag "):
                # "tag "ä»¥é™ã‚’ã‚¿ã‚°åã¨ã—ã¦å–å¾—
                tag_name = command[4:].strip()
                # ã‚¿ã‚°åã§æ¤œç´¢
                results = analyzer.find_by_tag(tag_name)
                # çµæœæ•°ã‚’è¡¨ç¤º
                print(f"[DEBUG] {len(results)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            # cssã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            elif command.startswith("css "):
                # "css "ä»¥é™ã‚’ã‚»ãƒ¬ã‚¯ã‚¿ã¨ã—ã¦å–å¾—
                selector = command[4:].strip()
                # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢
                analyzer.find_by_css_selector(selector)
            
            # textã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            elif command == "text":
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                text = analyzer.extract_text()
                # æœ€åˆã®500æ–‡å­—ã®ã¿è¡¨ç¤º
                print(f"\n{text[:500]}...")
            
            # ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã®å ´åˆ
            else:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                print("[DEBUG] ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰")
        
        # çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\n" + "=" * 70)
        print("âœ… è§£æå®Œäº†")
        print("=" * 70)
        # ãƒ­ã‚°ã«çµ‚äº†ã‚’è¨˜éŒ²
        logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")
    
    except KeyboardInterrupt:  # Ctrl+CãŒæŠ¼ã•ã‚ŒãŸå ´åˆ
        # ä¸­æ–­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        print("\n\n[DEBUG] ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­")
    
    except Exception as e:  # ãã®ä»–ã®ä¾‹å¤–
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        print(f"\n[DEBUG] âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ä»˜ãï¼‰
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


# ============================================
# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ============================================

# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿main()ã‚’å®Ÿè¡Œ
if __name__ == "__main__":  # __name__ãŒ"__main__"ã®å ´åˆï¼ˆç›´æ¥å®Ÿè¡Œæ™‚ï¼‰
    # ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    main()
